{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis (EDA) for Food Reviews Dataset",
   "id": "8bb661b62f3d1dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Overview of the Datasets\n",
   "id": "4e6bde2cde4f365b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the datasets",
   "id": "445ca67f3ce29e2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV and TXT files\n",
    "bad_lines = []\n",
    "def collect_bad_line(line):\n",
    "    bad_lines.append(line)\n",
    "    return None  # skip\n",
    "\n",
    "nutrition_df = pd.read_csv('data/Nutrition.csv', sep=\";\")\n",
    "restaurants_df = pd.read_csv('data/Restaurants.csv', sep=\";\")\n",
    "reviews_df = pd.read_csv('data/Reviews.txt', sep=\"\\t\")\n",
    "recipes_df = pd.read_csv(\"data/Recipes.csv\", sep=\";\", engine=\"python\", on_bad_lines=collect_bad_line)\n",
    "\n",
    "print(\"loaded:\", recipes_df.shape)\n",
    "print(\"bad lines captured:\", len(bad_lines))\n",
    "print(\"example bad line:\", bad_lines[0][:2000] if bad_lines else \"none\")\n"
   ],
   "id": "3c9ebd378ebbff2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipes Dataset",
   "id": "47ca085326c00518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('====== Recipes Dataset ======')\n",
    "print('Number of rows: ', recipes_df.shape[0])\n",
    "print('Number of columns: ', recipes_df.shape[1])\n",
    "print('Number of Unique Recipe Categories: ', len(list(set(recipes_df['RecipeCategory'].unique()))))\n",
    "print('Number of Unique DatePublished: ', len(list(set(recipes_df['DatePublished'].unique()))))\n",
    "print('Number of duplicates: ', recipes_df.duplicated().sum())\n",
    "print('Number of Null Values: ', recipes_df.isnull().sum().sum())\n",
    "print('Number of rows with null values: ', len(recipes_df[recipes_df.isnull().any(axis=1)].index))\n",
    "print('Columns with Null Values: ', recipes_df.isnull().sum())\n",
    "print('\\n')\n",
    "recipes_df.head()\n"
   ],
   "id": "9ad34a5651175932"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nutrition Dataset\n",
   "id": "fdce41bbaeef5d09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('====== Nutrition Dataset ======')\n",
    "print('Number of rows: ', nutrition_df.shape[0])\n",
    "print('Number of columns: ', nutrition_df.shape[1])\n",
    "unique_recipe_categories = list(set(nutrition_df['RecipeCategory'].unique()))\n",
    "print('Number of Unique Recipe Categories: ', len(unique_recipe_categories))\n",
    "print(unique_recipe_categories)\n",
    "if 'Fast Food' in unique_recipe_categories: print('Hawaiian is in the list')\n",
    "print('Number of duplicates: ', nutrition_df.duplicated().sum())\n",
    "print('Number of Null Values: ', nutrition_df.isnull().sum().sum())\n",
    "print('Number rows with null values: ', [r for r in nutrition_df[nutrition_df.isnull().any(axis=1)].index])\n",
    "print('Columns with Null Values: ', nutrition_df.isnull().sum())\n",
    "print('\\n')\n",
    "nutrition_df.head()"
   ],
   "id": "e64e05a98c254a7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Restaurants Dataset",
   "id": "7ab3963c637a7b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('====== Restaurants Dataset ======')\n",
    "print('Number of rows: ', restaurants_df.shape[0])\n",
    "print('Number of columns: ', restaurants_df.shape[1])\n",
    "print('Number of Unique Countries: ', len(list(set(restaurants_df['Country'].unique()))))\n",
    "print('Number of Unique Cuisines: ', len(list(set(restaurants_df['Cuisines'].unique()))))\n",
    "print('Number of duplicates: ', restaurants_df.duplicated().sum())\n",
    "print('Number of Null Values: ', restaurants_df.isnull().sum().sum())\n",
    "print('The rows of the null values: ', [r for r in restaurants_df[restaurants_df.isnull().any(axis=1)].index])\n",
    "print('\\n')\n",
    "restaurants_df.head()"
   ],
   "id": "f0fc83b13a5315d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for index, row in restaurants_df.iterrows():\n",
    "    q = row['Cuisines']\n",
    "    for u in unique_recipe_categories:\n",
    "        if u == q:\n",
    "            print(f\"Found a match in row {index} with category {u} and cuisine {q}\")"
   ],
   "id": "8f3be6b0ff9de4d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reviews Dataset\n",
    "This is the unstructured dataset that contains the reviews from users. the reviews are supposed to be on a single line, however this is not the case and a single review may be split up into more than one line."
   ],
   "id": "af022cacc8a2178c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('====== Reviews Dataset ======')\n",
    "print('Number of rows: ', reviews_df.shape[0])\n",
    "print('Number of columns: ', reviews_df.shape[1])\n",
    "print('Number of Unique Authors: ', len(list(set(reviews_df['AuthorId'].unique()))))\n",
    "print('Number of duplicates: ', reviews_df.duplicated().sum())\n",
    "print('Number of Null Values: ', reviews_df.isnull().sum().sum())\n",
    "print('Number of rows with null values: ', len(reviews_df[reviews_df.isnull().any(axis=1)].index))\n",
    "print('Columns with Null Values: \\n', reviews_df.isnull().sum())\n",
    "print('\\n')\n",
    "reviews_df.head(20)"
   ],
   "id": "a31f7fa4b4f799f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reviews that have recipe ids that are not present in the recipes dataset\n",
    "invalid_reviews = reviews_df[reviews_df['RecipeId'].isin(recipes_df['RecipeId'])]\n",
    "print('Number of reviews with invalid recipe ids: ', invalid_reviews.shape[0])"
   ],
   "id": "5cabc28ef93cca68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleaning the Datasets\n",
    "With every single dataset we will have to clean it in order to make it suitable for the construction of the knowledge graph. In all datasets duplicated rows are removed. Null values are treated accordingly based on the importance of the attribute. Also, only recipes which have nutritional information are kept, therefore, keeping only recipes that are present in both the Recipes and Nutrition datasets.\n",
    "\n",
    "#### Relevant Columns in each dataset:\n",
    "- For the **Recipe dataset**, <u>remove</u> the `RecipeYield`, `RecipeServings`, and `Images` columns as they are not relevant to the task. Replace the null values in the `CookTime` column with the median cook time. Also, <u>remove</u> the rows that have the null values in the `RecipeCategory` and `Keywords` column as they are crucial attributes for the knowledge graph.\n",
    "\n",
    "- In the **Nutrition dataset**, <u>keep</u> all the columns as they are relevant to the construction of the knowledge graph. But <u>remove</u> the rows that do not have a `RecipeCategory` value and nutritional values.\n",
    "\n",
    "- In the **Restaurants dataset**, <u>remove</u> the `Is delivering now`, `Switch to order menu`, `Price range`, `Rating color`, `Rating text`, `Longitude`, `Latitude` and `Nummber of dishes in cuisines` columns as they are not relevant to the task.\n",
    "\n",
    "- The **Reviews dataset**, which is unstructured, is preprocessed to deal with all the reviews that are split into multiple lines and columns. All columns are <u>kept</u>."
   ],
   "id": "bb490dfdb40dc054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove duplicates from all the datasets\n",
    "recipes_df.drop_duplicates(inplace=True)\n",
    "restaurants_df.drop_duplicates(inplace=True)\n",
    "nutrition_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove irrelevant columns from the datasets\n",
    "recipes_df.drop(columns=[\"RecipeYield\", \"RecipeServings\", \"Images\"], inplace=True)\n",
    "restaurants_df.drop(columns=[\"Is delivering now\", \"Switch to order menu\", \"Price range\", \"Nummber of dishes in cuisines\", \"Rating color\", \"Rating text\", \"Longitude\", \"Latitude\"], inplace=True)\n",
    "\n",
    "# Remove rows with null values in the RecipeCategory and Keywords columns in the Recipes dataset\n",
    "recipes_df.dropna(subset=['RecipeCategory', 'Keywords', 'RecipeIngredientQuantities'], inplace=True)\n",
    "nutrition_df.dropna(inplace=True) # remove rows with null values in the Nutrition dataset\n",
    "\n",
    "# Remove all the recipes in the Recipes dataset that are not present in the Nutrition dataset\n",
    "recipes_df = recipes_df[recipes_df['Name'].isin(nutrition_df['Name'])]"
   ],
   "id": "8f7396a7cf63da97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean the cook and prep times of the Recipes dataset\n",
    "def clean_cook_time_column(df, column):\n",
    "    for index, row in df.iterrows(): # iterate through each row in the dataframe\n",
    "        minutes = 0 # initialise the minutes variable\n",
    "        time_str = row[column] # get the cook time string\n",
    "        if pd.isna(time_str): # if the cook time value is null, skip the row\n",
    "            df.at[index, column] = str(0)\n",
    "            continue\n",
    "        time_str = time_str.replace('PT', '') # remove the 'PT' prefix from the cook time values\n",
    "        for char in time_str: # iterate through each character in the cook time string\n",
    "            if char == \"H\":\n",
    "                minutes += int(time_str[:time_str.index(char)]) * 60 # add the hours to the minutes variable\n",
    "                time_str = time_str[time_str.index(char)+1:]\n",
    "            elif char == \"M\":\n",
    "                minutes += int(time_str[:time_str.index(char)]) # add the minutes to the minutes variable\n",
    "        df.at[index, column] = str(minutes) # replace the cook time string with the minutes variable\n",
    "    df[column] = pd.to_numeric(df[column], downcast='float') # convert the cook time values to numeric\n",
    "    df[column].fillna(df[column].median()) # replace null cook time values with median time\n",
    "\n",
    "# Convert CookTime and PrepTime to numeric and replace null values with median\n",
    "clean_cook_time_column(recipes_df, 'CookTime')\n",
    "clean_cook_time_column(recipes_df, 'PrepTime')"
   ],
   "id": "5fd4145a5c46d088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean the Reviews dataset",
   "id": "9929f2043c75fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# String Type Constants\n",
    "NL = 0\n",
    "DT = 1\n",
    "Z = 2\n",
    "NULL = 3\n",
    "\n",
    "\n",
    "def is_datetime_string(s):\n",
    "    if isinstance(s, str) and len(s) == 20 and s[4] == '-' and s[7] == '-':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_line_and_split(line):\n",
    "    # remove the double quotes from the line and split it by tab character\n",
    "    line = line.replace('\"\"\"', \"\").replace('\"\"', \"\").replace('\"', \"\")\n",
    "    split_line = line.split(\"\\t\") # split the line based on the tab character\n",
    "    split_line = [s.strip() for s in split_line] # remove leading and trailing whitespace from each element\n",
    "    return split_line\n",
    "\n",
    "def get_string_types_from_split_line(row: list[str]):\n",
    "    string_types = []\n",
    "    for i in row:\n",
    "        if i == '' or i is None or i.lower() == 'nan':\n",
    "            string_types.append(NULL)\n",
    "        elif is_datetime_string(i):\n",
    "            string_types.append(DT)\n",
    "        elif i.isdigit():\n",
    "            string_types.append(Z)\n",
    "        else:\n",
    "            string_types.append(NL)\n",
    "    return string_types\n",
    "\n",
    "# Check if a row is in the correct order of the column types\n",
    "def check_correct_row_order(row: list[str]):\n",
    "    st = get_string_types_from_split_line(row)\n",
    "    if len(st) == len(row) == 7 and st[0] == Z and st[1] == Z and st[2] == Z and st[3] == NL and st[4] == NL and st[5] == DT and st[6] == DT:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Function to make the row from an initial line. It iteratively builds and cleans rows until the correct order is achieved\n",
    "def make_row(all_lines, line_idx, line_string):\n",
    "    sl = clean_line_and_split(line_string)\n",
    "    # print(sl)\n",
    "    # Merge the reviews that have been split into multiple lines or columns\n",
    "    row = [None for _ in range(7)] # new row to store the cleaned data\n",
    "    review = \"\" # string to store the review text\n",
    "    date1, date2 = None, None # strings to store the first and second datetime\n",
    "    for n, col in enumerate(sl):\n",
    "        if col.isdigit() and n < 3: # the first three columns ReviewID, RecipeID, AuthorID should be numeric values\n",
    "                row[n] = col\n",
    "        elif n == 3 and not col.isdigit() and not is_datetime_string(col): # the fourth should be the AuthorName column\n",
    "            row[n] = col\n",
    "        else:\n",
    "            if not is_datetime_string(col):\n",
    "                review += \" \" + col\n",
    "            else:\n",
    "                if not date1 and col != '':\n",
    "                    date1 = col\n",
    "                elif not date2 and date1 and col != '':\n",
    "                    date2 = col\n",
    "    st_1 = get_string_types_from_split_line(sl)\n",
    "\n",
    "    # If the previous row had only natural language, empty values and integers and no datetime strings, then we can merge the next\n",
    "    # row with the current row depending on whether the next row has only datetime strings or natural language and empty values\n",
    "    next_row_idx = line_idx + 1\n",
    "    while date1 is None and date2 is None and next_row_idx < len(all_lines):\n",
    "        if st_1.count(NL) + st_1.count(NULL) + st_1.count(Z) == len(st_1):\n",
    "        # if not check_correct_row_order(sl):\n",
    "        #     print('next row needs to be checked')\n",
    "            # Get the next line\n",
    "            next_line = clean_line_and_split(all_lines[next_row_idx])\n",
    "            st_2 = get_string_types_from_split_line(next_line)\n",
    "            # If the next row ha sonly natural language, then merge the text into the review column\n",
    "            if st_2.count(NL) + st_2.count(NULL) == len(st_2):\n",
    "                review += \" \".join(next_line)\n",
    "            # If the next row has only datetime strings, then merge the datetime into the date columns\n",
    "            if st_2.count(DT) + st_2.count(NULL) == len(st_2):\n",
    "                extracted_dates = [i for i in next_line if is_datetime_string(i)]\n",
    "                date1 = extracted_dates[0]\n",
    "                date2 = extracted_dates[1] if len(extracted_dates) > 1 else date1\n",
    "            # If the next row has both natural language and datetime strings\n",
    "            if st_2.count(NL) + st_2.count(DT) + st_2.count(NULL) == len(st_2) and st_2.count(DT) != 0 and st_2.count(NL) != 0:\n",
    "                extracted_dates = [i for i in next_line if is_datetime_string(i)]\n",
    "                date1 = extracted_dates[0]\n",
    "                date2 = extracted_dates[1] if len(extracted_dates) > 1 else date1\n",
    "                review += \" \".join([i for i in next_line if not is_datetime_string(i) and i != '' and i is not None])\n",
    "        next_row_idx += 1\n",
    "\n",
    "    row[4] = review.strip()\n",
    "    row[5] = date1\n",
    "    row[6] = date2\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "clean_reviews = []\n",
    "with open('data/Reviews.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    # print('First 5 lines of the Reviews.txt file:')\n",
    "    for idx, line in enumerate(lines):\n",
    "        if idx == 0: continue # skip the header line\n",
    "        # print(f'======== Line {idx} =======')\n",
    "        row = make_row(lines, idx, line)\n",
    "        if check_correct_row_order(row):\n",
    "            clean_reviews.append(row)\n",
    "\n",
    "        # print(st_1)\n",
    "        # print(row)\n",
    "\n",
    "\n",
    "clean_reviews = pd.DataFrame(clean_reviews, columns=['ReviewId', 'RecipeId', 'AuthorId', 'AuthorName', 'Review', 'DateSubmitted', 'DateModified'])\n",
    "print(f'Reduced to {len(clean_reviews)} rows from {reviews_df.shape[0]}')\n",
    "print('Number of rows: ', clean_reviews.shape[0])\n",
    "print('Number of columns: ', clean_reviews.shape[1])\n",
    "print('Number of Unique Authors: ', len(list(set(clean_reviews['AuthorId'].unique()))))\n",
    "print('Number of duplicates: ', clean_reviews.duplicated().sum())\n",
    "print('Number of Null Values: ', clean_reviews.isnull().sum().sum())\n",
    "print('Number of rows with null values: ', len(clean_reviews[clean_reviews.isnull().any(axis=1)].index))\n",
    "print('Columns with Null Values: \\n', clean_reviews.isnull().sum())\n",
    "print('\\n')\n",
    "clean_reviews.head()\n"
   ],
   "id": "28645d673f84ca04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save datasets",
   "id": "44653bbe454fd64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T21:44:55.699528Z",
     "start_time": "2026-02-10T21:44:53.473253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recipes_df.to_csv('data/cleaned_recipes.csv', index=False)\n",
    "nutrition_df.to_csv('data/cleaned_nutrition.csv', index=False)\n",
    "restaurants_df.to_csv('data/cleaned_restaurants.csv', index=False)\n",
    "clean_reviews.to_csv('data/cleaned_reviews.csv', index=False)"
   ],
   "id": "ab4c3a57c341bf3",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
